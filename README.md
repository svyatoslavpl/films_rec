1) Эксперименты: Я провел исследование с целью выбора оптимальной метрики для нашей задачи, но столкнулся с трудностями, поскольку выбор метрики представлял собой сложную задачу. Поэтому я решил прибегнуть к другому подходу и проанализировал время, необходимое для выполнения рекомендаций на инференсе с использованием разных моделей.

В рамках исследования я использовал несколько моделей, включая "paraphrase-distilroberta-base-v1", "paraphrase-MiniLM-L6-v2", "stsb-roberta-base-v2", "distiluse-base-multilingual-cased-v2", "bert-base-nli-mean-tokens" и "roberta-base-nli-mean-tokens". Я произвел замеры времени инференса для каждой модели и обнаружил, что время между моделями не сильно отличается.

Это означает, что с точки зрения производительности и времени выполнения, мы можем выбрать любую из этих моделей без значительных негативных последствий.

2) Запуск сервиса:
docker build -t my_app_image .
docker run -p 8501:8501 my_app_image

